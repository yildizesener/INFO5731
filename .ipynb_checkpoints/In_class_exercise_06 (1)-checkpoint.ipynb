{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7TahL04sVvR"
   },
   "source": [
    "# **The sixth in-class-exercise (20 points in total, 10/14/2020)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejyZITr8sjnh"
   },
   "source": [
    "## **1. Rule-based information extraction (10 points)**\n",
    "\n",
    "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvR_O9D8sOUY"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:06:00.495246Z",
     "start_time": "2020-10-15T16:05:59.809694Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:06:05.449300Z",
     "start_time": "2020-10-15T16:06:01.196931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "20 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "30 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "40 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "50 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "60 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "70 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "80 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "90 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n",
      "100 titles being extracted\n",
      "Everything is fine. Continue to scrap ...\n"
     ]
    }
   ],
   "source": [
    "headers = [{'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0'},\n",
    "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Vivaldi/3.3'},\n",
    "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'},\n",
    "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}]\n",
    "\n",
    "pages = np.arange(1, 11) * 10 \n",
    "\n",
    "responses = []\n",
    "for page in pages:\n",
    "    url = 'https://scholar.google.com/scholar?start=' + str(page) + '&q=natural+language+processing&hl=en&as_sdt=0,44'\n",
    "    print(f'{page} titles being extracted')\n",
    "    random_index = np.random.randint(0, 4)\n",
    "    header = headers[random_index]\n",
    "    response = requests.get(url, headers=header)\n",
    "    if response.status_code == 200:\n",
    "        print('Everything is fine. Continue to scrap ...')\n",
    "    else:\n",
    "        print('Something went wrong!')\n",
    "    soup=BeautifulSoup(response.content,'lxml')\n",
    "    responses.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:17:30.720980Z",
     "start_time": "2020-10-15T16:16:59.083459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------1-----------------------------\n",
      "-----------2-----------------------------\n",
      "-----------3-----------------------------\n",
      "-----------4-----------------------------\n",
      "-----------5-----------------------------\n",
      "-----------6-----------------------------\n",
      "-----------7-----------------------------\n",
      "-----------8-----------------------------\n",
      "-----------9-----------------------------\n",
      "-----------10-----------------------------\n",
      "-----------11-----------------------------\n",
      "-----------12-----------------------------\n",
      "-----------13-----------------------------\n",
      "-----------14-----------------------------\n",
      "-----------15-----------------------------\n",
      "-----------16-----------------------------\n",
      "-----------17-----------------------------\n",
      "-----------18-----------------------------\n",
      "-----------19-----------------------------\n",
      "-----------20-----------------------------\n",
      "-----------21-----------------------------\n",
      "-----------22-----------------------------\n",
      "-----------23-----------------------------\n",
      "-----------24-----------------------------\n",
      "-----------25-----------------------------\n",
      "-----------26-----------------------------\n",
      "-----------27-----------------------------\n",
      "-----------28-----------------------------\n",
      "-----------29-----------------------------\n",
      "-----------30-----------------------------\n",
      "-----------31-----------------------------\n",
      "-----------32-----------------------------\n",
      "-----------33-----------------------------\n",
      "-----------34-----------------------------\n",
      "-----------35-----------------------------\n",
      "-----------36-----------------------------\n",
      "-----------37-----------------------------\n",
      "-----------38-----------------------------\n",
      "-----------39-----------------------------\n",
      "-----------40-----------------------------\n",
      "-----------41-----------------------------\n",
      "-----------42-----------------------------\n",
      "-----------43-----------------------------\n",
      "-----------44-----------------------------\n",
      "Something went wrong!\n",
      "-----------45-----------------------------\n",
      "-----------46-----------------------------\n",
      "-----------47-----------------------------\n",
      "-----------48-----------------------------\n",
      "-----------49-----------------------------\n",
      "-----------50-----------------------------\n",
      "-----------51-----------------------------\n",
      "-----------52-----------------------------\n",
      "-----------53-----------------------------\n",
      "-----------54-----------------------------\n",
      "-----------55-----------------------------\n",
      "-----------56-----------------------------\n",
      "-----------57-----------------------------\n",
      "-----------58-----------------------------\n",
      "-----------59-----------------------------\n",
      "-----------60-----------------------------\n",
      "-----------61-----------------------------\n",
      "-----------62-----------------------------\n",
      "-----------63-----------------------------\n",
      "-----------64-----------------------------\n",
      "-----------65-----------------------------\n",
      "-----------66-----------------------------\n",
      "-----------67-----------------------------\n",
      "-----------68-----------------------------\n",
      "-----------69-----------------------------\n",
      "-----------70-----------------------------\n",
      "-----------71-----------------------------\n",
      "-----------72-----------------------------\n",
      "-----------73-----------------------------\n",
      "-----------74-----------------------------\n",
      "-----------75-----------------------------\n",
      "-----------76-----------------------------\n",
      "-----------77-----------------------------\n",
      "-----------78-----------------------------\n",
      "-----------79-----------------------------\n",
      "-----------80-----------------------------\n",
      "-----------81-----------------------------\n",
      "-----------82-----------------------------\n",
      "-----------83-----------------------------\n",
      "-----------84-----------------------------\n",
      "-----------85-----------------------------\n",
      "-----------86-----------------------------\n",
      "-----------87-----------------------------\n",
      "-----------88-----------------------------\n",
      "-----------89-----------------------------\n",
      "-----------90-----------------------------\n",
      "-----------91-----------------------------\n",
      "-----------92-----------------------------\n",
      "-----------93-----------------------------\n",
      "-----------94-----------------------------\n",
      "-----------95-----------------------------\n",
      "-----------96-----------------------------\n",
      "-----------97-----------------------------\n",
      "-----------98-----------------------------\n",
      "-----------99-----------------------------\n",
      "-----------100-----------------------------\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "count = 1\n",
    "for response in responses:\n",
    "    for item in response.select('[data-lid]'):\n",
    "        try:\n",
    "            print('-----------' + str(count) + '-----------------------------')\n",
    "            titles.append(item.select('h3')[0].get_text())\n",
    "            abstracts.append(item.select('.gs_rs')[0].get_text())\n",
    "        except Exception as error:\n",
    "            print('Something went wrong!')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:19:55.099947Z",
     "start_time": "2020-10-15T16:19:50.356213Z"
    }
   },
   "outputs": [],
   "source": [
    "# add one np.nan for the missing abstract\n",
    "abstracts.insert(44, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:22:43.566724Z",
     "start_time": "2020-10-15T16:22:43.535474Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PDF][PDF] Natural language processing</td>\n",
       "      <td>Description/Abstract Natural Language Processi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allennlp: A deep semantic natural language pro...</td>\n",
       "      <td>This paper describes AllenNLP, a platform for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentiment analysis: Capturing favorability usi...</td>\n",
       "      <td>This paper illustrates a sentiment analysis ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natural language processing: an introduction</td>\n",
       "      <td>Objectives To provide an overview and tutorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Introduction to Arabic natural language proces...</td>\n",
       "      <td>This book provides system developers and resea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0             [PDF][PDF] Natural language processing   \n",
       "1  Allennlp: A deep semantic natural language pro...   \n",
       "2  Sentiment analysis: Capturing favorability usi...   \n",
       "3       Natural language processing: an introduction   \n",
       "4  Introduction to Arabic natural language proces...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Description/Abstract Natural Language Processi...  \n",
       "1  This paper describes AllenNLP, a platform for ...  \n",
       "2  This paper illustrates a sentiment analysis ap...  \n",
       "3  Objectives To provide an overview and tutorial...  \n",
       "4  This book provides system developers and resea...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'titles' : titles, 'abstracts' : abstracts})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-15T16:23:56.891510Z",
     "start_time": "2020-10-15T16:23:19.341074Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('titles_abstracts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dq_7VGmrsum4"
   },
   "source": [
    "## **2. Domain-specific information extraction (10 points)**\n",
    "\n",
    "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
    "\n",
    "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
    "\n",
    "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
    "\n",
    "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
    "\n",
    "(4) companies, e.g., “Lexpredict LLC”\n",
    "\n",
    "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
    "\n",
    "(6) constraints, e.g., “no more than”\n",
    "\n",
    "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
    "\n",
    "(8) courts, e.g., “Supreme Court of New York”\n",
    "\n",
    "(9) CUSIP, e.g., “392690QT3”\n",
    "\n",
    "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
    "\n",
    "(11) definitions, e.g., “Term shall mean …”\n",
    "\n",
    "(12) distances, e.g., “fifteen miles”\n",
    "\n",
    "(13) durations, e.g., “ten years” or “thirty days”\n",
    "\n",
    "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
    "\n",
    "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
    "\n",
    "(16) percents and rates, e.g., “10%” or “50 bps”\n",
    "\n",
    "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
    "\n",
    "(18) ratios, e.g.,” 3:1” or “four to three”\n",
    "\n",
    "(19) regulations, e.g., “32 CFR 170”\n",
    "\n",
    "(20) trademarks, e.g., “MyApp (TM)”\n",
    "\n",
    "(21) URLs, e.g., “http://acme.com/”\n",
    "\n",
    "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
    "\n",
    "(23) persons, e.g., “John Doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc7NtJrLx5tS"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAs8kIQ1wq7/rcR1WavISL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "In_class_exercise_05.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
