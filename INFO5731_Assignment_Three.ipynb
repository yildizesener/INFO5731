{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Three**\n",
    "\n",
    "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Understand N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\\\\n', '\\\\n', \"',\"), 108),\n",
       " ((\"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\", '\\\\n', '\\\\n'), 91),\n",
       " ((\"\\\\n']\", \"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\", '\\\\n'), 83),\n",
       " (('\\\\n', \"',\", \"'\\\\n\"), 81),\n",
       " ((\"'\\\\n\", \"\\\\n']\", \"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"), 61),\n",
       " ((\"'\\\\n\", 'stars', \"\\\\n']\"), 30),\n",
       " (('\\\\n', \"',\", '\"\\\\n'), 29),\n",
       " (('stars', \"\\\\n']\", \"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"), 27),\n",
       " (('stars', \"'\\\\n\", \"\\\\n']\"), 14),\n",
       " ((\"',\", \"'\\\\n\", 'I'), 12)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams \n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams# function for making ngrams\n",
    "\n",
    "#Write your code here\n",
    "with open('/Users/yildizesener/Desktop/reviews.txt') as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokenized = text.split()\n",
    "\n",
    "# and get a list of all the trigrams\n",
    "esTrigrams = ngrams(tokenized, 3)\n",
    "\n",
    "esTrigramsFreq = collections.Counter(esTrigrams)\n",
    "\n",
    "esTrigramsFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\\\\n', \"',\"), 110),\n",
       " (('of', '5'), 110),\n",
       " (('\\\\n', '\\\\n'), 108),\n",
       " ((\"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\", '\\\\n'), 93),\n",
       " ((\"\\\\n']\", \"['\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"), 90),\n",
       " ((\"',\", \"'\\\\n\"), 81),\n",
       " ((\"'\\\\n\", \"\\\\n']\"), 75),\n",
       " (('5.0', 'out'), 57),\n",
       " ((\"'\\\\n\", 'stars'), 30),\n",
       " (('stars', \"\\\\n']\"), 30)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = text.split()\n",
    "\n",
    "# and get a list of all the bigrams\n",
    "esbigrams = ngrams(tokenized, 2)\n",
    "\n",
    "esbigramsFreq = collections.Counter(esbigrams)\n",
    "\n",
    "esbigramsFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 18928),\n",
       " ('n', 3174),\n",
       " ('e', 2564),\n",
       " ('t', 2247),\n",
       " ('\\n', 1954),\n",
       " ('o', 1858),\n",
       " ('a', 1810),\n",
       " ('\\\\', 1766),\n",
       " ('s', 1600),\n",
       " ('i', 1388)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/yildizesener/Desktop/reviews.txt') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def find_bigrams(text):\n",
    "    bigram_list = []\n",
    "    for i in range(len(text)-1):\n",
    "        bigram_list.append((text[i], text[i+1]))\n",
    "    return text\n",
    "esbigramsFreq = collections.Counter( text)\n",
    "\n",
    "esbigramsFreq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ngrams = ngrams(text.split(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(text,n):\n",
    "\n",
    "    # split sentences into tokens\n",
    "    tokens=re.split(\"\\\\s+\",text)\n",
    "    ngrams=[]\n",
    "\n",
    "    # collect the n-grams\n",
    "    for i in range(len(tokens)-n+1):\n",
    "       temp=[tokens[j] for j in range(i,i+n)]\n",
    "       ngrams.append(\" \".join(temp))\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Undersand TF-IDF and Document representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
    "\n",
    "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# The GitHub link of your final csv file\n",
    "\n",
    "# Link: "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMniIalS+f3MyeuLTJeFDvi",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
